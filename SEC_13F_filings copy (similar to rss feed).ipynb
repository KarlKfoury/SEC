{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "\n",
    "%run Definitions.py\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect(\"/Users/ralph/Biotech/BiotechDatabase.db\") \n",
    "\n",
    "# def create_tables():\n",
    "#     # create a cursor\n",
    "#     cur = conn.cursor()\n",
    "\n",
    "#     # create tables\n",
    "#     create_table_commands = (\n",
    "#         \"\"\"\n",
    "#             CREATE TABLE filings (\n",
    "#                 filing_id varchar(255) PRIMARY KEY,\n",
    "#                 cik int,\n",
    "#                 filer_name varchar(255),\n",
    "#                 period_of_report date\n",
    "#             )\n",
    "#         \"\"\",\n",
    "#         \"\"\"\n",
    "#             CREATE TABLE holdings (\n",
    "#                 filing_id varchar(255),\n",
    "#                 name_of_issuer varchar(255),\n",
    "#                 cusip varchar(255),\n",
    "#                 title_of_class varchar(255),\n",
    "#                 value bigint,\n",
    "#                 shares int,\n",
    "#                 put_call varchar(255)\n",
    "#             )\n",
    "#         \"\"\", \n",
    "#         \"\"\"\n",
    "#             CREATE TABLE holding_infos (\n",
    "#                 cusip varchar(255),\n",
    "#                 security_name varchar(255),\n",
    "#                 ticker var(50),\n",
    "#                 exchange_code varchar(10),\n",
    "#                 security_type varchar(50)\n",
    "#             )\n",
    "#         \"\"\")\n",
    "\n",
    "#     # create table one by one\n",
    "#     for command in create_table_commands:\n",
    "#         cur.execute(command)\n",
    "    \n",
    "#     # close cursor\n",
    "#     cur.close()\n",
    "    \n",
    "#     # make the changes to the database persistent\n",
    "#     conn.commit()\n",
    "#     conn.close()\n",
    "\n",
    "\n",
    "\n",
    "# create_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#V2:\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def doGet(url):\n",
    "    s = requests.Session()\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "        'Sec-CH-UA': 'Examplary Browser',\n",
    "        'Sec-CH-UA-Mobile': '?0',\n",
    "        'Sec-CH-UA-Platform': \"Windows\",\n",
    "        \"sec-fetch-dest\": \"document\",\n",
    "        \"sec-fetch-mode\": \"navigate\",\n",
    "        \"sec-fetch-site\": \"none\",\n",
    "        \"sec-fetch-user\": \"?1\",\n",
    "        \"upgrade-insecure-requests\": \"1\",\n",
    "        \"authority\": \"www.sec.gov\",\n",
    "        \"method\": \"GET\",\n",
    "        \"path\": \"/Archives/edgar/data/59478/000120919121046268/0001209191-21-046268-index.htm\",\n",
    "        \"scheme\": \"https\",\n",
    "        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,/;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "        \"accept-encoding\": \"gzip deflate br\",\n",
    "        \"accept-language\": \"en-US,en;q=0.9,ar-LB;q=0.8,ar;q=0.7\",\n",
    "        \"cache-control\": \"max-age=0\"}\n",
    "\n",
    "\n",
    "    r = None\n",
    "    proxies = {\n",
    "        'http': 'socks5://127.0.0.1:9050',\n",
    "        'https': 'socks5://127.0.0.1:9050'\n",
    "    }\n",
    "    try:\n",
    "        r = s.get(url, headers=headers)\n",
    "\n",
    "        if r.status_code == 403:\n",
    "            i = 1\n",
    "            while r.status_code == 403 and i <= 10:\n",
    "                print(\"Status code: \" + str(r.status_code) + \"  for : \" + url)\n",
    "                time.sleep(1)\n",
    "                r = s.get(url, headers=headers, proxies = proxies)\n",
    "                i += 1\n",
    "        # print(\"Status code: \" + str(r.status_code) + \"  for : \" + url)\n",
    "        # r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(\"Http Error: \" + errh + \" for url: \" + url)\n",
    "        if r.get_status_code == 403:\n",
    "            i = 1\n",
    "            while r.status_code == 403 and i <= 10:\n",
    "                time.sleep(i)\n",
    "                r = s.get(url, headers=headers)\n",
    "                i += 1\n",
    "    except requests.exceptions.ConnectionError as errc:\n",
    "        print(\"Error Connecting: \" + str(errc) + \"  for : \" + url)\n",
    "    except requests.exceptions.Timeout as errt:\n",
    "        print(\"Timeout Error: \" + str(errt) + \"  for : \" + url)\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(\"OOps: Something Else\" + str(err) + \"  for : \" + url)\n",
    "\n",
    "    return r\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def doGet13F(url, payload=None):\n",
    "    s = requests.Session()\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "        'Sec-CH-UA': 'Examplary Browser',\n",
    "        'Sec-CH-UA-Mobile': '?0',\n",
    "        'Sec-CH-UA-Platform': \"Windows\",\n",
    "        \"sec-fetch-dest\": \"document\",\n",
    "        \"sec-fetch-mode\": \"navigate\",\n",
    "        \"sec-fetch-site\": \"none\",\n",
    "        \"sec-fetch-user\": \"?1\",\n",
    "        \"upgrade-insecure-requests\": \"1\",\n",
    "        \"authority\": \"www.sec.gov\",\n",
    "        \"method\": \"GET\",\n",
    "        \"path\": \"/Archives/edgar/data/59478/000120919121046268/0001209191-21-046268-index.htm\",\n",
    "        \"scheme\": \"https\",\n",
    "        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,/;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "        \"accept-encoding\": \"gzip deflate br\",\n",
    "        \"accept-language\": \"en-US,en;q=0.9,ar-LB;q=0.8,ar;q=0.7\",\n",
    "        \"cache-control\": \"max-age=0\"}\n",
    "\n",
    "    r = None\n",
    "    proxies = {\n",
    "        'http': 'socks5://127.0.0.1:9050',\n",
    "        'https': 'socks5://127.0.0.1:9050'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        if payload:\n",
    "            r = s.get(url, headers=headers, params=payload)\n",
    "        else:\n",
    "            r = s.get(url, headers=headers)\n",
    "\n",
    "        if r.status_code == 403:\n",
    "            i = 1\n",
    "            while r.status_code == 403 and i <= 10:\n",
    "                print(\"Status code: \" + str(r.status_code) + \" for: \" + url)\n",
    "                time.sleep(1)\n",
    "                if payload:\n",
    "                    r = s.get(url, headers=headers, params=payload, proxies=proxies)\n",
    "                else:\n",
    "                    r = s.get(url, headers=headers, proxies=proxies)\n",
    "                i += 1\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(\"Http Error: \" + str(errh) + \" for url: \" + url)\n",
    "        if r.status_code == 403:\n",
    "            i = 1\n",
    "            while r.status_code == 403 and i <= 10:\n",
    "                time.sleep(i)\n",
    "                if payload:\n",
    "                    r = s.get(url, headers=headers, params=payload)\n",
    "                else:\n",
    "                    r = s.get(url, headers=headers)\n",
    "                i += 1\n",
    "    except requests.exceptions.ConnectionError as errc:\n",
    "        print(\"Error Connecting: \" + str(errc) + \" for: \" + url)\n",
    "    except requests.exceptions.Timeout as errt:\n",
    "        print(\"Timeout Error: \" + str(errt) + \" for: \" + url)\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(\"OOps: Something Else\" + str(err) + \" for: \" + url)\n",
    "\n",
    "    return r\n",
    "\n",
    "# https://www.sec.gov/cgi-bin/browse-edgar?action=getcurrent&CIK=&type=13F-HR&company=&dateb=&owner=include&start=0&count=40&output=atom\n",
    "\n",
    "\n",
    "def download_filings(start_date, end_date):\n",
    "    base_url = \"https://www.sec.gov\"\n",
    "    search_url = f\"{base_url}/cgi-bin/browse-edgar\"\n",
    "\n",
    "    # Format the dates in yyyy-mm-dd format\n",
    "    start_date = start_date\n",
    "    end_date = end_date\n",
    "    start_page = 0\n",
    "    # Create the search payload with the specified parameters (try both getcurrent and getgeneral)\n",
    "    payload = {\n",
    "        \"action\": \"getcurrent\",\n",
    "        \"CIK\": \"\",\n",
    "        \"company\":\"\",\n",
    "        \"SIC\":\"\",\n",
    "        \"state\":\"\",\n",
    "        \"type\": \"13F-HR\",\n",
    "        \"datea\": start_date,\n",
    "        \"dateb\": end_date,\n",
    "        \"owner\": \"include\",\n",
    "        \"accno\":\"\",\n",
    "        \"output\": \"atom\",\n",
    "        \"count\": \"100\",\n",
    "        \"start\":start_page\n",
    "    }\n",
    "\n",
    "    # Store all the filings URLs\n",
    "    filings_urls = []\n",
    "\n",
    "    # Store the undownloaded filings URLs list\n",
    "    to_be_downloaded_filings = []\n",
    "\n",
    "\n",
    "    while True:\n",
    "        payload[\"start\"] = str(start_page)\n",
    "        response = doGet13F(search_url, payload=payload)\n",
    "        response_str = response.text\n",
    "        soup = BeautifulSoup(response_str, 'xml')\n",
    "\n",
    "        # Find all the entries and extract the filing URLs\n",
    "        entries = soup.find_all(\"entry\")\n",
    "        if not entries:\n",
    "            break\n",
    "\n",
    "        for entry in entries:\n",
    "            link = entry.find(\"link\", attrs={\"type\": \"text/html\"})\n",
    "            title = entry.find(\"title\")\n",
    "            filing_type = title.text.strip()\n",
    "            # print(f'Filing Type Before Verification: {filing_type}')\n",
    "            if filing_type.startswith(\"13F-HR\") and not filing_type.startswith(\"13F-HR/A\") and not filing_type.startswith(\"13F-NT\"):\n",
    "                # print(f'Filing Type Being Selected: {filing_type}')       \n",
    "                filings_urls.append(link.get(\"href\"))\n",
    "                # print(f'Filings URLs length: {len(filings_urls)}')\n",
    "\n",
    "\n",
    "        start_page += 100\n",
    "\n",
    "\n",
    "    print(f'Total filings from {start_date} to {end_date}: {len(filings_urls)}')\n",
    "\n",
    "    # Create the directory to save the filings\n",
    "    directory = \"data/13F\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    # processed_directory = 'data/13F_processed'\n",
    "    # # Create the processed directory if it doesn't exist\n",
    "    # os.makedirs(processed_directory, exist_ok=True)\n",
    "\n",
    "    # Screen for new filings\n",
    "    for filing_url in filings_urls:\n",
    "        filing_id = filing_url.split(\"/\")[-1].split(\".\")[0]\n",
    "        filing_id = filing_id.replace(\"-index\", \"\")\n",
    "\n",
    "        # Extract the year from the filing_id using regex\n",
    "        pattern = r'-(\\d{2})-'\n",
    "        match = re.search(pattern, filing_id)\n",
    "        \n",
    "        if match:\n",
    "            year_number = match.group(1)\n",
    "            filing_id = filing_id.split('.')[0]\n",
    "\n",
    "            # Convert the year number to a four-digit year\n",
    "            if int(year_number) >= 95 and int(year_number) <= 99:\n",
    "                year = 1900 + int(year_number)\n",
    "            else:\n",
    "                year = 2000 + int(year_number)\n",
    "\n",
    "        filename = f\"{directory}/{year}/{filing_id}.txt\"  # Updated filename with subdirectory\n",
    "\n",
    "        # Check if the filing has already been saved\n",
    "        if os.path.exists(filename):\n",
    "            # print(f\"Filing {filing_id} already downloaded. Skipping.\")\n",
    "            continue\n",
    "        else:\n",
    "            to_be_downloaded_filings.append((filing_url,year))\n",
    "\n",
    "    print(f'Total filings to be downloaded from {start_date} to {end_date}: {len(to_be_downloaded_filings)}')\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Download new filings:\n",
    "    for filing_data in to_be_downloaded_filings:\n",
    "        filing_url, year = filing_data  # Unpack the tuple\n",
    "        filing_id = filing_url.split(\"/\")[-1].split(\".\")[0]\n",
    "        filing_id = filing_id.replace(\"-index\", \"\")\n",
    "        filename = f\"{directory}/{year}/{filing_id}.txt\"\n",
    "        print(f'filing url is: {filing_url}')\n",
    "        response2 = doGet(filing_url)\n",
    "        # print(\"Response2:\", response2.status_code)  # Verify response status\n",
    "        response2_str = response2.text\n",
    "        soup = BeautifulSoup(response2_str, 'xml')\n",
    "\n",
    "        # Find the document link and download the filing\n",
    "        table = soup.find(\"table\", class_=\"tableFile\")\n",
    "        # print(f'Table: {table}')\n",
    "        rows = table.find_all(\"tr\")\n",
    "        # print(f'Rows: {rows}')\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            # print(f'Cells: {cells}')\n",
    "            # if len(cells) > 3 and cells[3].text.strip() == \"13F-HR\":\n",
    "            if len(cells) > 2 and \"Complete submission text file\" in cells[1].text:\n",
    "                document_link = cells[2].find(\"a\")\n",
    "                if document_link:\n",
    "                    document_url = base_url + document_link.get(\"href\")\n",
    "                    # print(f'Document URL: {document_url}')\n",
    "                    document_response = doGet(document_url)\n",
    "\n",
    "                    with open(filename, \"wb\") as file:\n",
    "                        file.write(document_response.content)\n",
    "\n",
    "                    # print(f\"Downloaded filing: {filing_id}\")\n",
    "                    break  # Only download the first document link found\n",
    "\n",
    "    print(\"All filings downloaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total filings from 2023-07-10 to 2023-07-21: 1286\n",
      "Total filings to be downloaded from 2023-07-10 to 2023-07-21: 0\n",
      "All filings downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# to download in multiple day batches (change your timedelta accordingly). \n",
    "# Problem is during filing period, too many filings will be available that exceed the number of pages the feed will be able to get\n",
    "def split_into_two_weeks(start_date, end_date):\n",
    "    date_format = \"%Y-%m-%d\"\n",
    "    start = datetime.strptime(start_date, date_format)\n",
    "    end = datetime.strptime(end_date, date_format)\n",
    "    delta = timedelta(days=14)\n",
    "\n",
    "    current_start = start\n",
    "    current_end = min(start + delta, end + timedelta(days=1))  # Ensure the last interval doesn't exceed the end date\n",
    "\n",
    "    while current_start <= end:\n",
    "        download_filings(current_start.strftime(date_format), current_end.strftime(date_format))\n",
    "\n",
    "        current_start += delta\n",
    "        current_end = min(current_start + delta, end + timedelta(days=0))\n",
    "\n",
    "# Example usage\n",
    "start_date = \"2023-07-10\"\n",
    "end_date = \"2023-07-20\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "split_into_two_weeks(start_date, end_date)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# download_filings(\"202-01-01\", \"2023-03-01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total filings from 2023-05-01 to 2023-05-01: 103\n",
      "Total filings to be downloaded from 2023-05-01 to 2023-05-01: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-02 to 2023-05-02: 216\n",
      "Total filings to be downloaded from 2023-05-02 to 2023-05-02: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-03 to 2023-05-03: 196\n",
      "Total filings to be downloaded from 2023-05-03 to 2023-05-03: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-04 to 2023-05-04: 247\n",
      "Total filings to be downloaded from 2023-05-04 to 2023-05-04: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-05 to 2023-05-05: 208\n",
      "Total filings to be downloaded from 2023-05-05 to 2023-05-05: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-06 to 2023-05-06: 0\n",
      "Total filings to be downloaded from 2023-05-06 to 2023-05-06: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-07 to 2023-05-07: 0\n",
      "Total filings to be downloaded from 2023-05-07 to 2023-05-07: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-08 to 2023-05-08: 225\n",
      "Total filings to be downloaded from 2023-05-08 to 2023-05-08: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-09 to 2023-05-09: 303\n",
      "Total filings to be downloaded from 2023-05-09 to 2023-05-09: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-10 to 2023-05-10: 339\n",
      "Total filings to be downloaded from 2023-05-10 to 2023-05-10: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-11 to 2023-05-11: 405\n",
      "Total filings to be downloaded from 2023-05-11 to 2023-05-11: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-12 to 2023-05-12: 794\n",
      "Total filings to be downloaded from 2023-05-12 to 2023-05-12: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-13 to 2023-05-13: 0\n",
      "Total filings to be downloaded from 2023-05-13 to 2023-05-13: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-14 to 2023-05-14: 0\n",
      "Total filings to be downloaded from 2023-05-14 to 2023-05-14: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-15 to 2023-05-15: 1696\n",
      "Total filings to be downloaded from 2023-05-15 to 2023-05-15: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-16 to 2023-05-16: 106\n",
      "Total filings to be downloaded from 2023-05-16 to 2023-05-16: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-17 to 2023-05-17: 27\n",
      "Total filings to be downloaded from 2023-05-17 to 2023-05-17: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-18 to 2023-05-18: 21\n",
      "Total filings to be downloaded from 2023-05-18 to 2023-05-18: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-19 to 2023-05-19: 26\n",
      "Total filings to be downloaded from 2023-05-19 to 2023-05-19: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-20 to 2023-05-20: 0\n",
      "Total filings to be downloaded from 2023-05-20 to 2023-05-20: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-21 to 2023-05-21: 0\n",
      "Total filings to be downloaded from 2023-05-21 to 2023-05-21: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-22 to 2023-05-22: 11\n",
      "Total filings to be downloaded from 2023-05-22 to 2023-05-22: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-23 to 2023-05-23: 9\n",
      "Total filings to be downloaded from 2023-05-23 to 2023-05-23: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-24 to 2023-05-24: 3\n",
      "Total filings to be downloaded from 2023-05-24 to 2023-05-24: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-25 to 2023-05-25: 5\n",
      "Total filings to be downloaded from 2023-05-25 to 2023-05-25: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-26 to 2023-05-26: 25\n",
      "Total filings to be downloaded from 2023-05-26 to 2023-05-26: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-27 to 2023-05-27: 0\n",
      "Total filings to be downloaded from 2023-05-27 to 2023-05-27: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-28 to 2023-05-28: 0\n",
      "Total filings to be downloaded from 2023-05-28 to 2023-05-28: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-29 to 2023-05-29: 0\n",
      "Total filings to be downloaded from 2023-05-29 to 2023-05-29: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-30 to 2023-05-30: 3\n",
      "Total filings to be downloaded from 2023-05-30 to 2023-05-30: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-05-31 to 2023-05-31: 10\n",
      "Total filings to be downloaded from 2023-05-31 to 2023-05-31: 0\n",
      "All filings downloaded successfully.\n",
      "Total filings from 2023-06-01 to 2023-06-01: 19\n",
      "Total filings to be downloaded from 2023-06-01 to 2023-06-01: 0\n",
      "All filings downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# To download day by day\n",
    "def split_into_days(start_date, end_date):\n",
    "    date_format = \"%Y-%m-%d\"\n",
    "    start = datetime.strptime(start_date, date_format)\n",
    "    end = datetime.strptime(end_date, date_format)\n",
    "    delta = timedelta(days=1)  # Change delta to 1 day\n",
    "\n",
    "    current_date = start\n",
    "\n",
    "    while current_date <= end:\n",
    "        download_filings(current_date.strftime(date_format), current_date.strftime(date_format))\n",
    "\n",
    "        current_date += delta\n",
    "\n",
    "# Example usage\n",
    "start_date = \"2023-05-01\"\n",
    "end_date = \"2023-06-01\"\n",
    "\n",
    "split_into_days(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
