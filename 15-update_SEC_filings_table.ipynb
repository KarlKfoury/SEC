{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import requests\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To download all data between two dates\n",
    "\n",
    "# import csv\n",
    "# import requests\n",
    "# import sqlite3\n",
    "\n",
    "# # Define the date range\n",
    "# start_date = 1995  # Starting year\n",
    "# end_date = 2023    # Ending year\n",
    "# user_agent = {\"User-agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# # Establish a connection to the SQLite database\n",
    "# conn = sqlite3.connect(\"/Users/ralph/Biotech/BiotechDatabase.db\") \n",
    "# cursor = conn.cursor()\n",
    "\n",
    "# # Create the SEC_filings table if it doesn't exist\n",
    "# cursor.execute('''CREATE TABLE IF NOT EXISTS SEC_filings (\n",
    "#                     accession VARCHAR(255) PRIMARY KEY,\n",
    "#                     cik INT,\n",
    "#                     comnam VARCHAR(255),\n",
    "#                     form VARCHAR(255),\n",
    "#                     date DATE,\n",
    "#                     url VARCHAR(255)\n",
    "#                 )''')\n",
    "\n",
    "# # Download and process the master.idx file for each year and quarter\n",
    "# with open(\"master.idx\", \"wb\") as f:\n",
    "#     for year in range(start_date, end_date + 1):\n",
    "#         for q in range(1, 5):\n",
    "#             print(year, q)\n",
    "#             content = requests.get(\n",
    "#                 f\"https://www.sec.gov/Archives/edgar/full-index/{year}/QTR{q}/master.idx\",\n",
    "#                 headers=user_agent,\n",
    "#             ).content\n",
    "#             f.write(content)\n",
    "\n",
    "#             # Write the downloaded content to the master.idx file\n",
    "#             f.write(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Q3 data of 2023\n",
      "Downloading Q2 data of 2023\n"
     ]
    }
   ],
   "source": [
    "# To import data for the last two quarters:\n",
    "\n",
    "# Define the date range\n",
    "current_year = datetime.datetime.now().year\n",
    "current_quarter = (datetime.datetime.now().month - 1) // 3 + 1\n",
    "previous_year = current_year if current_quarter > 1 else current_year - 1\n",
    "previous_quarter = current_quarter - 1 if current_quarter > 1 else 4\n",
    "\n",
    "user_agent = {\"User-agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# Establish a connection to the SQLite database\n",
    "conn = sqlite3.connect(\"/Users/ralph/Biotech/BiotechDatabase.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the SEC_filings table if it doesn't exist\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS SEC_filings (\n",
    "                    accession VARCHAR(255) PRIMARY KEY,\n",
    "                    cik INT,\n",
    "                    comnam VARCHAR(255),\n",
    "                    form VARCHAR(255),\n",
    "                    date DATE,\n",
    "                    url VARCHAR(255)\n",
    "                )''')\n",
    "\n",
    "# Download and process the master.idx file for the current and previous quarters\n",
    "with open(\"master.idx\", \"wb\") as f:\n",
    "    print(f'Downloading Q{current_quarter} data of {current_year}')\n",
    "    content_current = requests.get(\n",
    "        f\"https://www.sec.gov/Archives/edgar/full-index/{current_year}/QTR{current_quarter}/master.idx\",\n",
    "        headers=user_agent,\n",
    "    ).content\n",
    "    print(f'Downloading Q{previous_quarter} data of {previous_year}')\n",
    "    content_previous = requests.get(\n",
    "        f\"https://www.sec.gov/Archives/edgar/full-index/{previous_year}/QTR{previous_quarter}/master.idx\",\n",
    "        headers=user_agent,\n",
    "    ).content\n",
    "\n",
    "    # Save the content of both quarters in the master.idx file\n",
    "    f.write(content_current)\n",
    "    f.write(content_previous)\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows added: 0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Establish a connection to the SQLite database\n",
    "conn = sqlite3.connect(\"/Users/ralph/Biotech/BiotechDatabase.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Open and read the master.idx file\n",
    "with open(\"master.idx\", \"r\", encoding=\"latin1\") as file:\n",
    "    # Skip the header lines until the line containing \"CIK|Company Name|Form Type|Date Filed|Filename\"\n",
    "    for _ in range(5):\n",
    "        next(file)\n",
    "\n",
    "    # Create the CSV reader\n",
    "    reader = csv.reader(file, delimiter='|')\n",
    "\n",
    "    # Find the first row after the header\n",
    "    first_row = None\n",
    "    for row in reader:\n",
    "        if len(row) < 5:\n",
    "            continue  # Skip rows with insufficient values\n",
    "\n",
    "        if row[0] == \"CIK\":\n",
    "            continue  # Skip the header row\n",
    "\n",
    "        first_row = row\n",
    "        break\n",
    "\n",
    "    # # Print the first row of the reader\n",
    "    # if first_row:\n",
    "    #     print(\"First row:\", first_row)\n",
    "\n",
    "    # Batch processing variables\n",
    "    batch_size = 1000  # Number of rows per batch\n",
    "    batch_rows = []  # Rows to be inserted in a batch\n",
    "    total_rows_added = 0  # Total number of rows added\n",
    "\n",
    "    # Iterate over the remaining rows in master.idx\n",
    "    for row in reader:\n",
    "        if len(row) < 5:\n",
    "            continue  # Skip rows with insufficient values\n",
    "\n",
    "        if row[0] == \"CIK\":\n",
    "            continue  # Skip the header row if encountered again\n",
    "\n",
    "        cik = int(row[0])\n",
    "        comnam = row[1]\n",
    "        form = row[2]\n",
    "        date = row[3]\n",
    "        url = row[4]\n",
    "        accession = url.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "        # Create a tuple representing the row data\n",
    "        row_data = (accession, cik, comnam, form, date, url)\n",
    "        batch_rows.append(row_data)\n",
    "\n",
    "        # Check if the batch size is reached\n",
    "        if len(batch_rows) >= batch_size:\n",
    "            # Insert the batch of rows into the SEC_filings table, ignoring duplicates\n",
    "            cursor.executemany(\"INSERT OR IGNORE INTO SEC_filings (accession, cik, comnam, form, date, url) VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "                               batch_rows)\n",
    "            num_rows_added = cursor.rowcount  # Get the number of rows added in this batch\n",
    "            total_rows_added += num_rows_added  # Update the total row count\n",
    "            # print(f\"Rows added: {num_rows_added}\")\n",
    "\n",
    "            batch_rows = []  # Clear the batch rows\n",
    "\n",
    "    # Insert any remaining rows in the last batch\n",
    "    if batch_rows:\n",
    "        cursor.executemany(\"INSERT OR IGNORE INTO SEC_filings (accession, cik, comnam, form, date, url) VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "                           batch_rows)\n",
    "        num_rows_added = cursor.rowcount  # Get the number of rows added in the last batch\n",
    "        total_rows_added += num_rows_added  # Update the total row count\n",
    "\n",
    "    print(f\"Total rows added: {total_rows_added}\")\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#V2:\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def doGet(url):\n",
    "    s = requests.Session()\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "        'Sec-CH-UA': 'Examplary Browser',\n",
    "        'Sec-CH-UA-Mobile': '?0',\n",
    "        'Sec-CH-UA-Platform': \"Windows\",\n",
    "        \"sec-fetch-dest\": \"document\",\n",
    "        \"sec-fetch-mode\": \"navigate\",\n",
    "        \"sec-fetch-site\": \"none\",\n",
    "        \"sec-fetch-user\": \"?1\",\n",
    "        \"upgrade-insecure-requests\": \"1\",\n",
    "        \"authority\": \"www.sec.gov\",\n",
    "        \"method\": \"GET\",\n",
    "        \"path\": \"/Archives/edgar/data/59478/000120919121046268/0001209191-21-046268-index.htm\",\n",
    "        \"scheme\": \"https\",\n",
    "        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,/;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "        \"accept-encoding\": \"gzip deflate br\",\n",
    "        \"accept-language\": \"en-US,en;q=0.9,ar-LB;q=0.8,ar;q=0.7\",\n",
    "        \"cache-control\": \"max-age=0\"}\n",
    "\n",
    "\n",
    "    r = None\n",
    "    proxies = {\n",
    "        'http': 'socks5://127.0.0.1:9050',\n",
    "        'https': 'socks5://127.0.0.1:9050'\n",
    "    }\n",
    "    try:\n",
    "        r = s.get(url, headers=headers)\n",
    "\n",
    "        if r.status_code == 403:\n",
    "            i = 1\n",
    "            while r.status_code == 403 and i <= 10:\n",
    "                print(\"Status code: \" + str(r.status_code) + \"  for : \" + url)\n",
    "                time.sleep(1)\n",
    "                r = s.get(url, headers=headers, proxies = proxies)\n",
    "                i += 1\n",
    "        # print(\"Status code: \" + str(r.status_code) + \"  for : \" + url)\n",
    "        # r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(\"Http Error: \" + errh + \" for url: \" + url)\n",
    "        if r.get_status_code == 403:\n",
    "            i = 1\n",
    "            while r.status_code == 403 and i <= 10:\n",
    "                time.sleep(i)\n",
    "                r = s.get(url, headers=headers)\n",
    "                i += 1\n",
    "    except requests.exceptions.ConnectionError as errc:\n",
    "        print(\"Error Connecting: \" + str(errc) + \"  for : \" + url)\n",
    "    except requests.exceptions.Timeout as errt:\n",
    "        print(\"Timeout Error: \" + str(errt) + \"  for : \" + url)\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(\"OOps: Something Else\" + str(err) + \"  for : \" + url)\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2023 - Downloaded 13D Filings: 3097. Remaining: 21\n",
      "0\n",
      "Year: 2023\n",
      "Number of files deleted: 0\n",
      "Year 2023 - Downloaded 13D Filings: 3118. Remaining: 0\n",
      "Number of files deleted: 0\n",
      "Year 2023 - Downloaded 13G Filings: 21972. Remaining: 6\n",
      "0\n",
      "Year: 2023\n",
      "Number of files deleted: 0\n",
      "Year 2023 - Downloaded 13F Filings: 15111. Remaining: 123\n",
      "0\n",
      "Year: 2023\n",
      "Number of files deleted: 0\n",
      "Year 2023 - Downloaded 13F Filings: 15234. Remaining: 0\n",
      "Number of files deleted: 0\n"
     ]
    }
   ],
   "source": [
    "# Assign the desired values for filing and directory\n",
    "filings = [\"13D\",\"13G\",\"13F\"]\n",
    "\n",
    "for filing in filings:\n",
    "    directory = f\"data/{filing}\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # # Get a list of all subfolders\n",
    "    # subfolders = sorted([f for f in os.listdir(directory) if os.path.isdir(os.path.join(directory, f))], reverse = True)\n",
    "    \n",
    "    # to get the years of the last 2 quarters updated - otherwise use the subfolders code above to update the whole database\n",
    "    subfolders = sorted([str(current_year),str(previous_year)], reverse = True)\n",
    "    subfolders = [folder for folder in subfolders if folder] #remove empty values\n",
    "\n",
    "    # Iterate over the subfolders\n",
    "    for subfolder in subfolders:\n",
    "        # print(f'Looking at {filing} filings from year {subfolder}')\n",
    "        subdirectory = os.path.join(directory, subfolder)\n",
    "        # processed_subdirectory = os.path.join(processed_directory, subfolder)\n",
    "        # # Create the processed subdirectory if it doesn't exist\n",
    "        # os.makedirs(processed_subdirectory, exist_ok=True)\n",
    "        \n",
    "        # Create a list of filingIDs from the data directory\n",
    "        filingIDs = []\n",
    "        for filename in os.listdir(subdirectory):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                filingID = os.path.splitext(filename)[0]\n",
    "                filingIDs.append(filingID)\n",
    "        # print(f'Filings already downloaded in year {subfolder}: {len(filingIDs)}')\n",
    "\n",
    "\n",
    "        # Connect to the SQLite database and find already parsed filings\n",
    "        conn = sqlite3.connect(\"/Users/ralph/Biotech/BiotechDatabase.db\") \n",
    "        cursor = conn.cursor()\n",
    "        # Get a list of 13G filingIDs from the SQL table\n",
    "        select_query = f\"SELECT accession FROM SEC_{filing}_filings WHERE date LIKE '{subfolder}%'\"\n",
    "        cursor.execute(select_query)\n",
    "\n",
    "        # Fetch all rows and extract the accessions\n",
    "        sql_filingIDs = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "        filings_to_download = list(set(sql_filingIDs) - set(filingIDs))\n",
    "        filings_to_delete = list(set(filingIDs) - set(sql_filingIDs))\n",
    "\n",
    "        # print(f'Filings to be downloaded in year {subfolder}: {len(filings_to_download)}')\n",
    "        print(f'Year {subfolder} - Downloaded {filing} Filings: {len(filingIDs)}. Remaining: {len(filings_to_download)}')\n",
    "\n",
    "        # Create a comma-separated string of filings_to_download for the accessions\n",
    "        filings_to_download_str = ', '.join(['?' for _ in filings_to_download])\n",
    "\n",
    "        # Prepare the SELECT query\n",
    "        select_query = f\"SELECT accession, cik, comnam, form, date, url FROM SEC_{filing}_filings WHERE accession IN ({filings_to_download_str})\"\n",
    "        # Execute the query with the list of accessions\n",
    "        cursor.execute(select_query, filings_to_download)\n",
    "\n",
    "        # Fetch all the rows returned by the query\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "        # Get the column names from the cursor description\n",
    "        columns = [column[0] for column in cursor.description]\n",
    "\n",
    "        # Convert the rows into a list of dictionaries\n",
    "        rows = [dict(zip(columns, row)) for row in rows]\n",
    "\n",
    "        url_fails = []\n",
    "        previous_year = \"\"\n",
    "        deleted_count = 0\n",
    "\n",
    "        for n, row in enumerate(rows):\n",
    "            if n % 1000 == 0:\n",
    "                print('Parsing batch number '+str())\n",
    "            cik = row[\"cik\"]\n",
    "            date = row[\"date\"].strip()\n",
    "            year = row[\"date\"].split(\"-\")[0].strip()\n",
    "            # month = row[\"date\"].split(\"-\")[1].strip()\n",
    "            url = row[\"url\"].strip()\n",
    "            accession = url.split(\".\")[0].split(\"/\")[-1]\n",
    "            Path(f\"./{directory}/{year}\").mkdir(parents=True, exist_ok=True)\n",
    "            file_path = f\"./{directory}/{year}/{accession}.txt\"\n",
    "            # processed_file_path = f\"./{processed_directory}/{year}/{accession}.txt\"\n",
    "\n",
    "            if year != previous_year:\n",
    "                print(f'Year: {year}')\n",
    "                previous_year = year\n",
    "\n",
    "            if os.path.exists(file_path):# or os.path.exists(processed_file_path):\n",
    "                if os.path.getsize(file_path) == 0:\n",
    "                    os.remove(file_path)\n",
    "                    deleted_count += 1\n",
    "                    try:\n",
    "                        document_url = f\"https://www.sec.gov/Archives/{url}\"\n",
    "                        # print(f'Document URL: {document_url}')\n",
    "                        document_response = doGet(document_url)\n",
    "                        if document_response.content:\n",
    "                            with open(file_path, \"wb\") as file:\n",
    "                                file.write(document_response.content)\n",
    "                                pass\n",
    "                        else:\n",
    "                            display(f\"{cik}, {accession}, {date} failed to download - Empty content\")\n",
    "                            url_fails.append(row)\n",
    "                            pass\n",
    "                    except:\n",
    "                        display(f\"{cik}, {accession}, {date} failed to download\")\n",
    "                        url_fails.append(row)\n",
    "                        pass\n",
    "                    pass\n",
    "\n",
    "                else:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        try:\n",
    "                            first_line = f.readline()\n",
    "                        except:\n",
    "                            print(file_path, Exception)\n",
    "                            pass      \n",
    "                        \n",
    "                    if first_line.startswith('<!DOCTYPE html'):\n",
    "                        os.remove(file_path)\n",
    "                        deleted_count += 1\n",
    "\n",
    "                        try:\n",
    "                            document_url = f\"https://www.sec.gov/Archives/{url}\"\n",
    "                            # print(f'Document URL: {document_url}')\n",
    "                            document_response = doGet(document_url)\n",
    "                            if document_response.content:\n",
    "                                with open(file_path, \"wb\") as file:\n",
    "                                    file.write(document_response.content)\n",
    "                                    pass\n",
    "                            else:\n",
    "                                display(f\"{cik}, {accession}, {date} failed to download - Empty content\")\n",
    "                                url_fails.append(row)\n",
    "                                pass\n",
    "                        except:\n",
    "                            display(f\"{cik}, {accession}, {date} failed to download\")\n",
    "                            url_fails.append(row)\n",
    "                            pass\n",
    "                        pass\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    document_url = f\"https://www.sec.gov/Archives/{url}\"\n",
    "                    # print(f'Document URL: {document_url}')\n",
    "                    document_response = doGet(document_url)\n",
    "                    if document_response.content:\n",
    "                        with open(file_path, \"wb\") as file:\n",
    "                            file.write(document_response.content)\n",
    "                    else:\n",
    "                        display(f\"{cik}, {accession}, {date} failed to download - Empty content\")\n",
    "                        url_fails.append(row)\n",
    "                        pass\n",
    "\n",
    "                except:\n",
    "                    display(f\"{cik}, {accession}, {date} failed to download\")\n",
    "                    url_fails.append(row)\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"Number of files deleted: {deleted_count}\")\n",
    "\n",
    "        # Close the connection\n",
    "        conn.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
